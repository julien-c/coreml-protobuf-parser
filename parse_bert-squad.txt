Model {
  specificationVersion: 4,
  description: ModelDescription {
    input: [
      FeatureDescription {
        name: 'word_id',
        shortDescription: 'Sequence of input symbols. The sequence starts with a start token (101) followed by question tokens that are followed be a separator token (102) and the document tokens.The document tokens end with a separator token (102) and the sequenceis padded with 0 values to length 384.',
        type: FeatureType {
          multiArrayType: ArrayFeatureType {
            shape: [
              Long { low: 1, high: 0, unsigned: false },
              Long { low: 384, high: 0, unsigned: false }
            ],
            dataType: 65600
          }
        }
      },
      FeatureDescription {
        name: 'word_type',
        shortDescription: 'Sequence of token-types. Values of 0 for the start token, question tokens and the question separator. Value 1 for the document tokens and the end separator. The sequence is padded with 0 values to length 384.',
        type: FeatureType {
          multiArrayType: ArrayFeatureType {
            shape: [
              Long { low: 1, high: 0, unsigned: false },
              Long { low: 384, high: 0, unsigned: false }
            ],
            dataType: 65600
          }
        }
      },
      FeatureDescription {
        name: 'position',
        shortDescription: 'Fixed sequence of values from 0 to 383.',
        type: FeatureType {
          multiArrayType: ArrayFeatureType {
            shape: [
              Long { low: 1, high: 0, unsigned: false },
              Long { low: 384, high: 0, unsigned: false }
            ],
            dataType: 65600
          }
        }
      },
      FeatureDescription {
        name: 'attention_mask',
        shortDescription: 'A masking matrix (logits). It has zero values in the first X number of columns, where X = number of input tokens without the padding,and value -1e+4 in the remaining 384-X (padding) columns.',
        type: FeatureType {
          multiArrayType: ArrayFeatureType {
            shape: [
              Long { low: 1, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false },
              Long { low: 384, high: 0, unsigned: false },
              Long { low: 384, high: 0, unsigned: false }
            ],
            dataType: 65600
          }
        }
      }
    ],
    output: [
      FeatureDescription {
        name: 'start_logits',
        shortDescription: 'Start token scores of shape (1, 1, 384, 1). The argmax of the third axis is the start index of the predicted answer in the input sequence',
        type: FeatureType {
          multiArrayType: ArrayFeatureType { shape: [], dataType: 65600 }
        }
      },
      FeatureDescription {
        name: 'end_logits',
        shortDescription: 'End token scores of shape (1, 1, 384, 1). The argmax of the third axis is the end index of the predicted answer in the input sequence',
        type: FeatureType {
          multiArrayType: ArrayFeatureType { shape: [], dataType: 65600 }
        }
      }
    ],
    trainingInput: [],
    metadata: Metadata {
      userDefined: {},
      shortDescription: "Find the answer to a given question within a given document. The model was fine-tuned to question-answering using the project https://github.com/huggingface/pytorch-pretrained-BERT and is based on the pre-trained BERT model variant 'bert-base-uncased' (https://github.com/google-research/bert).",
      author: 'BERT Paper: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova',
      license: 'Please see https://github.com/google-research/bert for license information for the pre-trained BERT model, and https://rajpurkar.github.io/SQuAD-explorer/ for license information for the Question-Answering dataset, and https://github.com/huggingface/pytorch-pretrained-BERT for Question-Answering task model fine-tuning license information.'
    }
  },
  neuralNetwork: NeuralNetwork {
    layers: [
      NeuralNetworkLayer {
        input: [ 'word_id' ],
        output: [ 'word_id_transposed' ],
        inputTensor: [],
        outputTensor: [],
        name: 'word_id_transposed',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 0, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'word_id_transposed' ],
        output: [ 'word_id_expanded_to_rank5' ],
        inputTensor: [],
        outputTensor: [],
        name: 'word_id_expanded_to_rank5',
        expandDims: ExpandDimsLayerParams {
          axes: [
            Long { low: -3, high: -1, unsigned: false },
            Long { low: -2, high: -1, unsigned: false },
            Long { low: -1, high: -1, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'word_id_expanded_to_rank5' ],
        output: [ 'word_id_embedding' ],
        inputTensor: [],
        outputTensor: [],
        name: 'word_id_embedding',
        embedding: EmbeddingLayerParams {
          inputDim: Long { low: 30522, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 9c a0 fc a1 0d a5 b8 a4 a8 a1 8d a1 33 a2 8b a3 30 a3 07 a4 d2 a1 04 a4 4e a0 55 a4 2d a5 e4 9e 29 a4 61 a5 fd a0 3f a4 81 a5 38 a1 5f a2 5b a2 24 a5 ... 46881742 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'word_type' ],
        output: [ 'word_type_transposed' ],
        inputTensor: [],
        outputTensor: [],
        name: 'word_type_transposed',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 0, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'word_type_transposed' ],
        output: [ 'word_type_expanded_to_rank5' ],
        inputTensor: [],
        outputTensor: [],
        name: 'word_type_expanded_to_rank5',
        expandDims: ExpandDimsLayerParams {
          axes: [
            Long { low: -3, high: -1, unsigned: false },
            Long { low: -2, high: -1, unsigned: false },
            Long { low: -1, high: -1, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'word_type_expanded_to_rank5' ],
        output: [ 'word_type_embedding' ],
        inputTensor: [],
        outputTensor: [],
        name: 'word_type_embedding',
        embedding: EmbeddingLayerParams {
          inputDim: Long { low: 2, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 09 92 f8 19 f7 20 d1 98 9f 16 ae 98 f6 13 5c 99 21 96 21 9f 14 a1 01 14 72 1e a4 1d 64 9a 13 85 d4 9e 93 9d bf 23 80 8d 8a 19 12 23 13 25 9a 24 9f 15 ... 3022 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'position' ],
        output: [ 'position_transposed' ],
        inputTensor: [],
        outputTensor: [],
        name: 'position_transposed',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 0, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'position_transposed' ],
        output: [ 'position_expanded_to_rank5' ],
        inputTensor: [],
        outputTensor: [],
        name: 'position_expanded_to_rank5',
        expandDims: ExpandDimsLayerParams {
          axes: [
            Long { low: -3, high: -1, unsigned: false },
            Long { low: -2, high: -1, unsigned: false },
            Long { low: -1, high: -1, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'position_expanded_to_rank5' ],
        output: [ 'position_embedding' ],
        inputTensor: [],
        outputTensor: [],
        name: 'position_embedding',
        embedding: EmbeddingLayerParams {
          inputDim: Long { low: 512, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 7c 24 c1 1f a7 a0 86 9d 4f 9e 43 9c 39 1c 11 96 7c 09 00 1e 03 a0 42 a1 ea a3 3d a5 3e 9d 17 a1 60 a4 1a a1 8c 9f 2a 09 f2 1f 2d 12 d8 20 42 9b d0 a0 ... 786382 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [
          'word_id_embedding',
          'word_type_embedding',
          'position_embedding'
        ],
        output: [ 'added_embeddings' ],
        inputTensor: [],
        outputTensor: [],
        name: 'adding_embeddings',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ 'added_embeddings' ],
        output: [ 'added_embeddings_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: 'added_embeddings_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ 'added_embeddings_mvn' ],
        output: [ 'embed_out_seq_first' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_seq_first',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 68 3b 17 3b db 3a db 3a 26 3b 28 3b 72 3b 49 3b 83 3b 76 3a 67 3a 6f 3a 39 3b 89 3a 4e 3b ca 3a 87 3a e0 3a 4f 3b eb 3a 62 3b ea 3a 37 3b ee 3a be 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 55 a7 2c a5 6a 26 bd 2d 18 ac 5a a2 3f 21 f2 a3 f1 1b a0 28 33 aa 31 a4 a2 1c b4 ae c4 ac 73 27 bd a4 90 a9 6a ab 7f ad a1 a0 89 a8 bb a9 66 a5 ba 24 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_seq_first' ],
        output: [ 'embed_out' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 4, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out' ],
        output: [ 'embed_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 19 a4 80 26 28 a7 b7 28 5a a5 fb 2a 95 22 75 25 c9 19 b9 14 33 20 1b 28 e5 28 e6 1f c9 aa 9f 1d f9 2a 9b 28 a2 2a f0 a9 85 a9 1d 29 7c 2d 3d 25 bd a8 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer b3 38 43 b5 df b6 f9 35 c8 b4 8c 36 a9 22 b7 34 7d 33 04 b0 5c 30 3b 37 4a ae 15 2f e6 36 82 38 db 27 db a9 14 b5 df af 44 1f e0 1e a5 19 8b 37 79 a6 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query' ],
        output: [ 'embed_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: 'embed_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query_heads_rank5' ],
        output: [ 'embed_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query_heads_rank4' ],
        output: [ 'embed_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out' ],
        output: [ 'embed_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer c8 21 d3 a8 ac 24 58 29 ca 27 92 ac a2 2b 59 25 b8 a4 9c 13 13 24 0b a7 34 a2 b2 a0 1a 10 f3 1c 6f 27 60 28 46 21 c3 23 48 29 dc 23 a8 a1 3b a8 4a 21 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 53 14 b5 0d 09 99 c3 86 e4 94 83 19 ab 18 de 12 95 18 00 13 d4 13 c8 15 3b 11 d6 96 14 9c f0 17 fa 0a ca 9e e6 93 c9 95 dc 19 e7 1b 3c 90 21 13 d3 10 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_key' ],
        output: [ 'embed_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: 'embed_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_key_heads_rank5' ],
        output: [ 'embed_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_key_heads_rank4' ],
        output: [ 'embed_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out' ],
        output: [ 'embed_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 89 22 38 18 04 a1 b8 1e 15 29 96 a7 66 22 45 9d ec a7 24 28 52 a7 76 a8 7e 0d 29 a6 41 a7 3f a8 dd 92 ec a4 cd a4 33 24 28 aa 10 a3 3a 26 92 a6 92 27 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer f4 9f d0 a4 fc 95 46 24 32 1b 5e 22 0b a8 8d 23 54 22 10 1f 47 a6 31 29 49 9c 88 a0 ce 20 de 9d 32 26 b9 1c 98 a0 3a 24 e0 a8 37 21 0e a5 f7 24 44 1a ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_value' ],
        output: [ 'embed_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: 'embed_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_value_heads_rank5' ],
        output: [ 'embed_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_value_heads_rank4' ],
        output: [ 'embed_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query_heads', 'embed_out_key_heads' ],
        output: [ 'embed_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query_key_product' ],
        output: [ 'embed_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query_key_product_scaled', 'attention_mask' ],
        output: [ 'embed_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_query_key_product_masked' ],
        output: [ 'embed_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          'embed_out_query_key_product_normalized',
          'embed_out_value_heads'
        ],
        output: [ 'embed_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_qkv' ],
        output: [ 'embed_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_qkv_T' ],
        output: [ 'embed_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: 'embed_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_qkv_folded' ],
        output: [ 'embed_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_self_attn_out' ],
        output: [ 'embed_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 68 1d a0 92 2d 18 79 21 52 9a 2c 27 ea 20 cd a4 05 0d 15 26 62 a6 ab a5 6a 26 29 98 a4 21 29 21 02 23 66 98 39 a6 18 89 ba 9d db 2b 91 28 be a3 c5 1c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 64 1c 32 a4 34 27 79 a1 c0 24 7c a9 db a4 15 1f 7d a3 63 22 1b 26 4e 2a 95 2a 58 a8 db a4 3e 28 3b 28 07 a2 0d 9d 3a 9c 4b 94 ca 24 2c a5 a3 24 d7 14 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out', 'embed_out_self_attn_out_dense' ],
        output: [ 'embed_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_plus_self_attn' ],
        output: [ 'embed_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: 'embed_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ 'embed_out_plus_self_attn_mvn' ],
        output: [ '0_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer da 3b b0 3b b7 3b a5 3b d7 3b df 3b bc 3b c5 3b a0 3b 92 3b 91 3b 63 3b bf 3b 0a 3c c8 3b 92 3b 9d 3b be 3b af 3b 0d 3c a3 3b fa 3b ae 3b b1 3b af 3b ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 1b 34 cd a7 74 b4 33 b6 e0 35 6c b7 22 38 33 32 22 1f 3a 18 fb 2a a2 33 21 b8 f2 38 7a 30 59 b1 b0 b6 b7 b6 b0 2d d8 38 d0 34 07 b8 96 35 85 34 1b b6 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_attention_out' ],
        output: [ '0_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer c1 a0 78 98 bd 21 be 28 9f a2 7e 0f de 27 ff aa a5 a6 ca a4 ae 26 e5 a0 a5 a6 14 a6 12 a3 ef 92 70 21 6f 24 91 26 d3 a2 44 1f 2b 2c 9e a0 2e 28 b9 28 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 5e af 23 ae 05 b0 22 b0 23 ac dd b0 d8 ae a0 af 48 b0 15 b0 b6 25 5a ae c5 b0 e6 ae 3d a4 bc a7 f2 ad 86 ad e6 af 31 af 12 af 12 ae d2 ac 3f b0 1e b0 ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_attention_out_ff1' ],
        output: [ '0_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '0_attention_out_gelu' ],
        output: [ '0_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer a2 a8 05 a5 f2 26 61 24 0a 2c b8 a4 8c a8 56 28 79 a3 73 1f 2e a4 90 24 c0 2d 2a 18 4b 22 cb 1f 6b a2 7c a5 07 a3 76 23 4e 9d 00 a1 1f 24 b9 a2 8d 23 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 44 aa 4a 32 21 25 e2 27 84 29 2a 29 e3 aa 22 2b 86 24 72 2d 42 a2 84 28 fd 28 e6 ae 09 b0 8d 2c 6c 33 de aa 6c aa 00 a9 ad ab 2b ae 22 a4 38 ab b4 23 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_attention_out', '0_attention_out_ff2' ],
        output: [ '0_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '0_attention_out_dense2_plus_input' ],
        output: [ '0_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '0_attention_out_dense2_plus_input_mvn' ],
        output: [ '0_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 45 3a 76 3a 26 3a db 39 1b 3a 0e 3a 22 3a 3e 3a 59 3a 0c 3a ec 39 d4 39 d9 39 97 39 3c 3a f7 39 0c 3a db 39 66 3a c8 39 1a 3a f8 39 13 3a 27 3a dd 39 ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 8e ae 8f 9d 4d 29 18 2e f0 ae 15 2d 0c b2 e4 ad 21 a6 35 aa c0 a9 dc ae 51 30 c1 b2 f4 af dd 24 81 30 c5 2c 5a ad 2f b4 e8 af be 2b db b1 d9 af 1c 2b ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out' ],
        output: [ '0_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer b7 27 59 a6 74 18 b9 23 35 a8 85 1c b6 2b 43 21 7c 16 7a 2b 9e 1f d9 2b d8 24 bd a2 d0 23 9d a5 6e 28 ed 28 38 21 a2 a7 13 a8 3e 2e 97 9a d2 25 c7 a5 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 6d b4 7e 31 07 b4 0d bb 92 36 f3 22 12 38 4a 34 82 36 cf 28 1c 3b 8e 37 dc 22 78 ba cd 36 8a 1f 2f b4 d7 31 09 35 0b 2f 84 34 61 b5 33 39 c0 9d f8 39 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query' ],
        output: [ '0_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '0_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query_heads_rank5' ],
        output: [ '0_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query_heads_rank4' ],
        output: [ '0_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out' ],
        output: [ '0_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 58 a8 5b 28 95 a8 47 23 f3 1c 94 2c b7 ae c0 22 3c a5 c9 22 68 aa 11 25 65 27 d1 1a 29 2c 3d a3 8f a0 3c 10 34 29 e1 a0 1a 19 9b a4 b0 ad 98 23 40 ac ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 1e 9d 61 15 0b 9c 93 1e 61 96 96 97 be 10 24 0c ec 0d f1 15 75 11 94 92 00 14 2d 1a e1 10 83 9d 18 95 4b 16 45 14 ec 15 56 9b 55 95 88 98 a2 1c bb 95 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_key' ],
        output: [ '0_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '0_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_key_heads_rank5' ],
        output: [ '0_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_key_heads_rank4' ],
        output: [ '0_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out' ],
        output: [ '0_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 76 9b b3 21 62 24 31 a0 d4 20 c6 21 6a 21 d5 a2 32 0c 22 a5 2c 26 20 a9 9d 29 01 1d f1 aa 26 aa dd aa c0 27 b3 24 f1 28 6f 25 ef a6 9a a8 f6 ab 9b 25 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 59 1a 08 27 28 a0 75 9a 90 a2 94 a6 61 2e 58 23 77 9f 8c a2 00 18 f6 a8 10 a4 2a a1 a2 25 8f a7 d2 a2 c5 a6 ec a6 08 27 62 25 a2 97 52 22 d7 a8 21 1e ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_value' ],
        output: [ '0_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '0_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_value_heads_rank5' ],
        output: [ '0_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_value_heads_rank4' ],
        output: [ '0_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query_heads', '0_layer_out_key_heads' ],
        output: [ '0_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query_key_product' ],
        output: [ '0_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '0_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_query_key_product_masked' ],
        output: [ '0_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '0_layer_out_query_key_product_normalized',
          '0_layer_out_value_heads'
        ],
        output: [ '0_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_qkv' ],
        output: [ '0_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_qkv_T' ],
        output: [ '0_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '0_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_qkv_folded' ],
        output: [ '0_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_self_attn_out' ],
        output: [ '0_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer a6 a2 73 28 9f a8 31 a1 07 1a 06 24 33 22 95 24 51 24 af 13 64 1d 48 21 c1 a8 4e a8 f0 a6 6a a1 b7 0b a2 a5 1d 25 21 a7 48 28 e9 22 de 24 36 a9 81 2a ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 4a 27 1d 2b 8c 22 62 25 24 1d 9e a7 59 ac a7 0d 95 24 9d ab 28 1f 5f 2d 2f 2b 3f af 35 26 25 2e 52 29 bc a7 98 24 1d a8 b8 ae e9 a4 d4 a9 a7 22 b2 1b ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out', '0_layer_out_self_attn_out_dense' ],
        output: [ '0_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_plus_self_attn' ],
        output: [ '0_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '0_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '0_layer_out_plus_self_attn_mvn' ],
        output: [ '1_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 33 3b f4 3a d5 3a ed 3a 93 3b 64 3b 33 3b 23 3b 1e 3b f2 3a 9f 3a e1 3a e8 3a 36 3b 0b 3b f0 3a 6c 3a 15 3b 3b 3b c0 3b 0a 3b 1d 3b 38 3b 03 3b e2 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 6f 2d 72 30 8a ad f8 b1 a4 32 c1 b3 5c 34 44 31 f0 27 f9 2e 44 a5 a3 33 f3 b1 cb 34 22 9f 05 a3 51 af aa b4 05 ab 24 38 d0 32 e0 b5 49 32 72 2e a4 b1 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_attention_out' ],
        output: [ '1_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 74 24 31 25 e0 24 43 2b 19 12 6d 28 57 a8 3c a5 b6 ac dd 29 5e a9 90 2c f7 22 5e ab 98 25 45 a9 98 22 6e 23 e3 ae 27 20 7c a1 26 a5 05 a0 e2 24 ce 22 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer e2 b0 bf ae 6c ad 0a ae 7f ad b4 b0 f2 ae 23 b0 92 b0 90 ae aa af db ad ad b0 ee ae 48 2d 68 ae 84 ad 8d af 88 ac d9 ad a4 af 68 a9 15 ae 60 b0 01 af ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_attention_out_ff1' ],
        output: [ '1_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '1_attention_out_gelu' ],
        output: [ '1_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 57 a6 38 21 2c 29 22 24 ec 29 5d 25 69 2a d7 a4 7c 97 7e 28 42 9f 67 9d 13 aa c8 a9 ae a9 9e ab c8 28 4b ac 40 ab 76 15 23 29 c0 2b 5d a3 e8 2a 5d 9b ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer c4 a6 1d 2e 57 a7 dc 28 47 20 3a 26 d4 ab ec 2e e6 a5 d8 2e 39 29 a4 96 03 25 bb a7 fa ab 27 24 99 30 0b ab fc a4 ca a1 8d 1d c4 aa f6 a9 58 ab 44 ab ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_attention_out', '1_attention_out_ff2' ],
        output: [ '1_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '1_attention_out_dense2_plus_input' ],
        output: [ '1_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '1_attention_out_dense2_plus_input_mvn' ],
        output: [ '1_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3b 3b 14 3b 16 3b d8 3a fd 3a 03 3b ea 3a 53 3b 6c 3b 1c 3b c1 3a 5b 3a df 3a 72 3a 6c 3b dd 3a e4 3a 73 3a 84 3b 87 3a e6 3a c8 3a 2c 3b 29 3b 9d 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 06 ad ec ae 07 28 ec 2c 55 af ec 2f 0c b2 53 b0 6f ac 57 ac 15 ab 97 b0 59 2c ca b2 04 9d 8d a5 62 2c bb 2d e2 a6 4d b4 3f ae 8d 31 6d b1 85 ab ed 29 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out' ],
        output: [ '1_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 4f 2d c8 22 ff ab dd 2d f9 ab 01 2b de b0 b5 2c d0 1d fc 28 6d a9 a7 2f 2b ae e7 2c 80 a7 f7 a1 f8 28 42 2f 53 2c fb a4 e9 a4 b0 31 94 2f 7f 2b de 2c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer ce 28 a4 28 69 34 e2 b2 e2 aa 6e a9 ed ae 26 31 fa 32 14 2b 37 2f ff b0 8b 35 08 34 63 a8 49 b0 3f ae 8b 28 8a 34 0c 26 ab 2e 13 b1 a6 b4 e3 32 ee b3 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query' ],
        output: [ '1_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '1_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query_heads_rank5' ],
        output: [ '1_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query_heads_rank4' ],
        output: [ '1_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out' ],
        output: [ '1_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 15 2a 2a 2d 48 2c 56 ab ef ae 96 a9 9d 2c 32 9d 5d 25 75 a9 ca a0 9f 2d 46 ad d5 a3 21 b1 ff a9 f1 30 98 ad e7 a6 ec 2c f5 ad 40 2d 15 2b 49 aa 21 2a ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 28 8e 28 95 b3 94 77 96 18 0e 90 96 45 95 61 18 57 19 93 0b 28 89 11 80 e3 17 99 91 df 14 ff 0f 75 8c 3f 12 f2 18 4a 19 b9 15 33 14 79 14 2c 8a 34 14 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_key' ],
        output: [ '1_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '1_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_key_heads_rank5' ],
        output: [ '1_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_key_heads_rank4' ],
        output: [ '1_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out' ],
        output: [ '1_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 32 a1 28 a8 ea 24 3e a6 17 25 d2 98 82 22 71 1d 66 25 97 96 07 a5 9e 22 ab a4 5f 2a c5 a4 ff a4 cb 1f b2 9d 10 1e 13 22 3a 2c 29 9f e6 a1 10 24 c2 23 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 23 a7 4c 2a 47 2d 3d 29 d0 38 81 2f 93 aa 13 ac 47 a7 5d a9 f3 2c 01 2d 27 8b a6 31 64 aa f4 b0 65 a5 b5 22 94 25 86 a6 14 b2 07 ad c5 2a 35 28 5b a8 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_value' ],
        output: [ '1_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '1_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_value_heads_rank5' ],
        output: [ '1_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_value_heads_rank4' ],
        output: [ '1_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query_heads', '1_layer_out_key_heads' ],
        output: [ '1_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query_key_product' ],
        output: [ '1_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '1_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_query_key_product_masked' ],
        output: [ '1_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '1_layer_out_query_key_product_normalized',
          '1_layer_out_value_heads'
        ],
        output: [ '1_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_qkv' ],
        output: [ '1_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_qkv_T' ],
        output: [ '1_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '1_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_qkv_folded' ],
        output: [ '1_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_self_attn_out' ],
        output: [ '1_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 77 1d da 21 bb a9 9c 9f 89 9f ee 25 f1 22 bc a4 e9 21 6b 1f 7e a0 2f 1a 8d a7 04 a6 01 a7 5b 29 5f 20 e4 a7 f9 a4 12 9e 71 9c 95 20 62 21 8f 27 9c 20 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer f4 23 9b 2f 85 ab 59 2c 6d 1d 4f 1c b6 aa 81 25 3d a8 39 29 19 26 2c 20 73 2d 69 b2 de 28 c3 2c 27 22 a8 ae 1b 28 cd ae a9 b1 6a a5 32 a5 32 af 71 a9 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out', '1_layer_out_self_attn_out_dense' ],
        output: [ '1_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_plus_self_attn' ],
        output: [ '1_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '1_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '1_layer_out_plus_self_attn_mvn' ],
        output: [ '2_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 34 3b 1d 3b e5 3a db 3a a9 3b 51 3b 3b 3b 2b 3b 1e 3b db 3a 8b 3a e1 3a df 3a 0f 3b 1b 3b f2 3a 56 3a bc 3a 22 3b ba 3b 71 3b 36 3b 1d 3b 20 3b 9e 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer b9 30 b8 2f 3f b2 3f b4 2e 35 38 b4 e9 33 eb 2b a5 ac 57 25 aa 22 01 35 4f b4 6b 36 9b a8 3a 29 57 b1 3f b0 4c b0 c4 36 67 36 06 b6 bb 31 2d 33 b3 af ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_attention_out' ],
        output: [ '2_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer c1 a4 44 a1 85 a4 c5 2c f5 94 2f 24 be 9d 2a a4 af 24 e3 1b 08 aa 0e a7 3f a5 f9 9b 5d 20 27 2a 2c 21 85 a0 f6 26 05 25 52 21 b3 25 5c a8 b1 24 d2 29 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 7a ad 12 b0 4e af 14 ae 03 a6 16 b0 5c b0 ed a4 29 ac 88 af 71 b0 9a a7 13 b0 f4 ad f1 ad 98 ae 94 b0 50 ae 9e b0 20 ac 18 af e2 b0 2e b4 af ae 29 b0 ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_attention_out_ff1' ],
        output: [ '2_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '2_attention_out_gelu' ],
        output: [ '2_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer af ac 1f 28 c4 2a 5c 28 f0 a9 81 29 7c 27 49 24 8a 24 2a 95 74 29 06 a8 3f a4 25 2c 3d 28 37 28 f0 28 80 20 03 a6 fe 0e d3 a5 cf a4 4c a8 60 a7 8e 1c ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3b ac 7c 2b 2a a8 9a 1f e1 2b aa a3 4f 18 d9 2d 15 1e 3f 2a d7 24 63 2a 29 26 9f 25 52 ae 20 24 79 30 d3 ab d6 94 74 a9 1d ac 6b ad e3 ab d1 ac ff a5 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_attention_out', '2_attention_out_ff2' ],
        output: [ '2_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '2_attention_out_dense2_plus_input' ],
        output: [ '2_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '2_attention_out_dense2_plus_input_mvn' ],
        output: [ '2_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 0f 3b df 3a ce 3a d4 3a a8 3a e4 3a c9 3a 3a 3b 46 3b d7 3a 95 3a 1f 3a 81 3a 2d 3a 0b 3b cf 3a f7 3a a6 3a 3f 3b 65 3a a6 3a 73 3a ff 3a 03 3b a5 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 8b af a3 af f3 2c d2 2c 93 b0 30 30 c4 b0 0b ad ab a4 f6 aa a0 a9 dd b1 92 2e 5a b3 59 9f 8f aa 88 2b 0b 28 ac 24 16 b4 7f b1 3a 31 97 b0 b4 ad 60 29 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out' ],
        output: [ '2_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 9d 2b ce a7 c5 19 ab a8 56 a8 f5 29 f1 16 29 26 04 a9 7d ac 6e a1 4f 2b 4f 29 67 27 31 a7 3e ab 2b 25 34 a9 e2 29 72 25 ff a9 4d a4 a3 27 27 2b 7c ac ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 17 2e 39 b2 21 29 a9 31 0c ad ba 33 1a af c9 ac 69 30 d8 b3 5c 32 20 b5 58 28 9c 38 c6 ab 96 a6 3a 2e 08 aa a7 b2 c1 98 d0 34 52 2a 03 a8 9a 33 1d 32 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query' ],
        output: [ '2_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '2_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query_heads_rank5' ],
        output: [ '2_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query_heads_rank4' ],
        output: [ '2_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out' ],
        output: [ '2_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 56 27 4e 21 75 a9 d2 2f 2d a8 a2 2c b8 27 09 9b 10 a4 27 a6 d6 8e 87 28 17 99 48 26 a4 1f a9 ac b7 ab 24 28 9e 25 70 a9 6c a6 65 a6 49 28 fb 2c 1d 2a ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer c1 17 c0 9b 39 a1 f8 20 94 93 b2 a0 dd 9e 03 9f 60 14 be 21 07 91 83 a3 24 97 ee 89 19 9e 56 98 ab 9b 96 20 cf 11 b1 a0 b0 1f 36 13 bc 1e 0d 9f f7 1c ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_key' ],
        output: [ '2_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '2_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_key_heads_rank5' ],
        output: [ '2_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_key_heads_rank4' ],
        output: [ '2_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out' ],
        output: [ '2_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 8e 9d 53 24 b9 28 57 ac 49 1d fa a1 1e ab 84 00 0e a6 e7 29 88 a6 f0 26 c7 a8 f1 20 ca 27 e2 2a 89 98 15 ac dc a8 05 28 2d a1 10 a5 0f a6 c8 25 35 2d ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 05 2a d1 2a c2 1f 93 1e d5 22 b3 ab 15 a9 96 2a ab a6 29 a1 94 2c b8 1c 06 a6 09 a9 bf a7 23 0c 52 27 41 9a 46 9d 11 ab 27 ad ec 20 2a a7 4f a5 94 ab ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_value' ],
        output: [ '2_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '2_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_value_heads_rank5' ],
        output: [ '2_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_value_heads_rank4' ],
        output: [ '2_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query_heads', '2_layer_out_key_heads' ],
        output: [ '2_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query_key_product' ],
        output: [ '2_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '2_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_query_key_product_masked' ],
        output: [ '2_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '2_layer_out_query_key_product_normalized',
          '2_layer_out_value_heads'
        ],
        output: [ '2_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_qkv' ],
        output: [ '2_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_qkv_T' ],
        output: [ '2_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '2_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_qkv_folded' ],
        output: [ '2_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_self_attn_out' ],
        output: [ '2_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 02 25 54 a5 1f a9 73 a1 26 9d 8c 23 ee 19 9c a0 6e 27 37 22 4b 1d 01 a2 ce 24 98 a8 84 19 56 24 d7 a4 4d 85 f6 11 4e a7 0c 20 99 9f 68 9d 9e 24 25 aa ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer e5 24 ab 28 75 aa a9 27 1e 2a ad 28 5f a8 3a 21 ca aa 9a 27 7e ae cf 29 9b 2e ec a0 0c 24 c6 2d 9a a7 c7 2d 07 a8 e8 a9 4c b1 f6 af 60 ad 66 9e b0 28 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out', '2_layer_out_self_attn_out_dense' ],
        output: [ '2_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_plus_self_attn' ],
        output: [ '2_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '2_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '2_layer_out_plus_self_attn_mvn' ],
        output: [ '3_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 53 3b 17 3b a9 3a bd 3a 9e 3b 3c 3b 54 3b 27 3b 15 3b d7 3a 45 3a c5 3a da 3a a4 3a 1e 3b eb 3a 51 3a 85 3a 3c 3b a9 3b 8d 3b e2 3a 35 3b 30 3b bb 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 52 2e 2b 17 cc a2 3d b4 28 33 b0 b4 e6 33 63 2f ae a6 9b ac fb a7 04 35 37 b4 93 32 b5 ac e3 a0 46 b0 1e b0 ba ab 75 36 2c 36 85 b4 bc 30 29 33 a4 ab ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_attention_out' ],
        output: [ '3_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 64 a6 23 a8 02 1d ed 2a c5 a7 bf 2b ed a7 c9 9c 11 22 28 1c 99 15 9b 2a a7 aa 8a 25 7b a8 3c 30 c1 2b 7b 29 61 20 d6 18 16 a4 73 29 98 a3 5d 9f f2 ad ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3b af be b0 2c ad 39 22 0c ae e5 ae 96 ae a7 ad e8 b0 fd ad 7f af 43 b0 ec a5 37 2d 3e b0 02 af 3e af 8b ae df af b2 ab fd ad b4 2c 27 b1 e1 af b7 ad ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_attention_out_ff1' ],
        output: [ '3_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '3_attention_out_gelu' ],
        output: [ '3_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 06 a5 92 a1 a4 28 2d 9c 02 28 a8 2a e3 a4 a6 29 b3 26 0e 28 d2 a5 32 af b7 24 80 2b 62 1d 0a aa cb 18 aa a6 88 aa 93 2c ce a2 ca 28 42 ad 29 a6 76 90 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer d1 a8 74 2d 30 a9 cb 24 12 30 e4 a7 f8 20 af 2e b7 98 93 a6 d9 aa c2 2e 91 2c 30 1e c6 a6 4f 28 74 2d ba a7 4c a8 9d a1 82 20 c8 af 6e a8 4d ab 55 aa ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_attention_out', '3_attention_out_ff2' ],
        output: [ '3_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '3_attention_out_dense2_plus_input' ],
        output: [ '3_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '3_attention_out_dense2_plus_input_mvn' ],
        output: [ '3_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer a1 3a a6 3a 97 3a 8c 3a 77 3a 7b 3a 6c 3a cf 3a d7 3a 87 3a 42 3a e4 39 49 3a 27 3a b8 3a 70 3a 70 3a 6b 3a b9 3a e8 39 35 3a 2b 3a b0 3a 97 3a 61 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 21 ad 52 ac 5c a8 80 2a 5e ae b9 2c 53 b0 dd ad 95 a5 b3 a4 5d a5 4d b0 ef 2b 2f b0 2e a0 0a a5 e3 20 00 25 f0 06 41 b3 0a b1 05 2e 73 af 38 ad aa a5 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out' ],
        output: [ '3_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 33 2d d4 2a 99 25 e9 b0 f7 a1 1d ac bc 17 c2 a9 02 25 54 26 bf a8 5b ab 08 2c fb 27 ad aa 2e aa 15 a7 31 28 2a 20 5e a4 3c 9b 10 1c e7 a2 c6 a6 4d aa ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 5f 2c 34 b0 51 aa 54 1a 0b 30 c7 a5 fc 1e c5 9a 15 ad 11 b3 e4 ac 7e 2e 23 2e f2 b0 c0 a8 38 29 72 ac b5 ab 19 2f 7f a8 f2 2e 9d b1 89 2e c5 25 08 29 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query' ],
        output: [ '3_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '3_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query_heads_rank5' ],
        output: [ '3_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query_heads_rank4' ],
        output: [ '3_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out' ],
        output: [ '3_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 82 2c 8a aa 15 24 d0 9c 8c 9e 62 a4 e4 26 69 a3 8e 9c 8e 2a f2 24 b7 a3 cb a4 75 24 00 a5 3f a9 2f 2a 01 2b a6 22 f8 a2 8f 1b 4f 2b 3e 17 9d a9 30 a9 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 0c 9e c2 98 c8 19 23 17 a0 9d 0e 97 94 9c e3 9b 6f 98 a6 1f 85 1c 48 15 5d 9b 52 17 6a 9e 99 9c f9 1e 93 1e 46 9d 2a 1c b4 0f 4c 18 87 1a 3b 10 e3 9c ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_key' ],
        output: [ '3_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '3_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_key_heads_rank5' ],
        output: [ '3_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_key_heads_rank4' ],
        output: [ '3_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out' ],
        output: [ '3_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 59 2c d3 96 da a9 74 23 db aa 85 24 d8 23 ab 29 68 19 95 a4 38 26 7f 9c 01 9e 61 9d 6f 19 db 21 b7 28 b5 a6 cf 2d 8c aa 35 25 6c 2a af 2a 1c a8 20 ac ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer ca 9e f5 a6 27 28 25 26 75 a9 c8 27 fe 23 ee 9c 64 25 6b 26 b7 1e 50 a4 e7 27 15 a5 4a 1d 4a 25 21 29 40 a2 e4 a1 a1 a0 8d 27 2f 9c 0e 98 2c a5 d4 a0 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_value' ],
        output: [ '3_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '3_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_value_heads_rank5' ],
        output: [ '3_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_value_heads_rank4' ],
        output: [ '3_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query_heads', '3_layer_out_key_heads' ],
        output: [ '3_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query_key_product' ],
        output: [ '3_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '3_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_query_key_product_masked' ],
        output: [ '3_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '3_layer_out_query_key_product_normalized',
          '3_layer_out_value_heads'
        ],
        output: [ '3_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_qkv' ],
        output: [ '3_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_qkv_T' ],
        output: [ '3_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '3_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_qkv_folded' ],
        output: [ '3_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_self_attn_out' ],
        output: [ '3_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 56 26 25 29 04 23 ba 27 69 a7 1a a5 80 a9 a9 26 e2 26 d5 24 6b a6 ac 26 2d 1f df ac c2 96 d5 26 fa a7 50 a6 34 a8 c5 1f 63 9c ef 1e 65 a5 6e 24 20 9c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 00 28 e6 1d ff a6 14 a8 de a0 f3 2a bf a8 44 a8 2c a8 88 25 8c 9e e9 a9 bf 2b 05 ac 42 a2 9e 2b 9d 1d a1 28 e2 1f d8 a8 f4 ae 36 05 8a aa 38 a1 1d 28 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out', '3_layer_out_self_attn_out_dense' ],
        output: [ '3_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_plus_self_attn' ],
        output: [ '3_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '3_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '3_layer_out_plus_self_attn_mvn' ],
        output: [ '4_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 19 3b f6 3a 7d 3a 6e 3a 97 3b ec 3a 18 3b 0f 3b f8 3a a8 3a 18 3a 30 3a 62 3a 25 3a c7 3a 9e 3a f4 39 1f 3a 3b 3b 3d 3b 3f 3b ab 3a f7 3a 2a 3b 51 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 41 21 28 1e 4c a7 20 b3 76 34 87 ae cd 32 7b 2a a5 21 8d ae 01 ae fa 32 26 ac 30 2e 86 24 28 2d 07 af d5 a7 d3 ac c1 33 df 33 3b b3 18 2f e9 34 16 a7 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_attention_out' ],
        output: [ '4_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 7b 9e 42 a6 38 28 3b 28 83 95 0d 98 cf 98 52 23 cf ac 4c a8 f6 21 01 ab c6 26 64 a5 ba 25 4d ac 38 98 dc 2c a9 19 aa 28 2f a9 45 95 57 a6 1c 24 d6 25 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 93 ad 6e af 66 af 6b 97 41 b0 38 ae e7 ad 6c b0 57 af 72 b2 3c b0 0d b0 7b b1 c8 a9 d5 b2 f9 a9 77 b0 0d b1 a1 ad 66 af f6 a9 bc b1 ad b0 1f af 99 af ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_attention_out_ff1' ],
        output: [ '4_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '4_attention_out_gelu' ],
        output: [ '4_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 19 ab 64 29 78 a2 5e 1d c7 a9 7a 9f 92 23 66 2c b9 26 6c a6 05 14 50 a5 36 a8 8c 29 ae a7 f6 ab ff a9 b0 25 8f a4 71 a4 9e 26 5e 22 9c a6 72 27 65 28 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 42 aa 66 2a f7 21 b7 2a 96 2d 9d 1c a4 a0 d0 2c 52 a5 ca a1 2f a6 7f 2c b7 8c d3 29 b2 a9 74 a4 8d a0 a7 8c a1 a0 2f a8 dc a0 28 ac 94 a9 82 aa c1 a5 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_attention_out', '4_attention_out_ff2' ],
        output: [ '4_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '4_attention_out_dense2_plus_input' ],
        output: [ '4_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '4_attention_out_dense2_plus_input_mvn' ],
        output: [ '4_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer ff 3a dc 3a ad 3a c2 3a 7d 3a d3 3a b1 3a 08 3b 09 3b f2 3a a4 3a 3f 3a 7b 3a 7a 3a fc 3a d5 3a a8 3a d9 3a 04 3b 7d 3a c0 3a 84 3a f3 3a d8 3a 9c 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 9d a9 e0 ac 39 a3 4b 2a 17 b0 f7 1f b9 af e6 ac a0 a6 65 23 54 9c 3e af cd a3 a1 ac ea a7 18 ab 4b a4 2e a5 58 9d 55 b1 52 b0 4d 2c 9c ae ef ae ac a7 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out' ],
        output: [ '4_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 85 a1 c2 18 55 28 ad aa d9 23 20 28 58 20 68 28 20 a7 d9 29 84 a3 73 18 a5 1b 65 2a 48 29 7f a9 74 28 6a 27 0d a9 97 a5 65 22 04 9c 86 a9 79 94 64 ab ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 89 a0 76 a4 d1 a9 72 21 cc 27 09 29 d9 28 01 30 93 9b a0 29 ed 2e 11 9e a1 a6 f9 a6 e1 2e 63 b5 06 26 c5 34 a3 28 9a 2f 86 9d 3a ac 12 ac 8f 25 82 2c ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query' ],
        output: [ '4_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '4_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query_heads_rank5' ],
        output: [ '4_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query_heads_rank4' ],
        output: [ '4_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out' ],
        output: [ '4_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 10 1d 04 27 37 aa 2f 27 db 28 2c 28 5d 25 18 2a 0d a8 fb aa af 2b d5 2c 9c 1f 77 29 8a 1e 20 28 6a 27 09 9d ec 9a fd 23 e4 24 18 aa 38 a5 73 ad f2 ab ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 91 a1 06 a2 a4 09 f5 a1 d9 20 e8 a1 e4 14 ad 9d fe 91 e4 1d dc 21 67 0e 21 91 0a 9c 28 98 11 a1 c7 93 f2 21 c1 1e 39 20 53 20 9e 9f 3a 9f 1b 21 01 14 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_key' ],
        output: [ '4_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '4_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_key_heads_rank5' ],
        output: [ '4_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_key_heads_rank4' ],
        output: [ '4_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out' ],
        output: [ '4_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer ff 2b 32 9b 2e a8 f2 a6 ac 14 2a 18 72 18 31 1e fc 29 0c a3 e2 20 bd a9 b7 a7 fa 29 c1 1e 4c a9 38 27 cb 2b ed 0c f7 29 36 20 7a 20 49 25 8e a7 2e a5 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 4c 2a 47 a8 6a 9b da 18 43 29 af a5 8f a0 41 24 52 29 fd a3 12 29 25 29 f0 23 e5 29 c6 22 b9 24 92 27 33 26 46 0f 26 a5 e3 9e 73 8b 4a a9 4e 2c 92 a3 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_value' ],
        output: [ '4_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '4_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_value_heads_rank5' ],
        output: [ '4_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_value_heads_rank4' ],
        output: [ '4_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query_heads', '4_layer_out_key_heads' ],
        output: [ '4_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query_key_product' ],
        output: [ '4_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '4_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_query_key_product_masked' ],
        output: [ '4_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '4_layer_out_query_key_product_normalized',
          '4_layer_out_value_heads'
        ],
        output: [ '4_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_qkv' ],
        output: [ '4_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_qkv_T' ],
        output: [ '4_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '4_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_qkv_folded' ],
        output: [ '4_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_self_attn_out' ],
        output: [ '4_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer de 9f a7 25 48 a8 aa 23 8b 99 0e 22 ea 27 42 1c 4b 23 52 24 c3 22 5a 24 26 a2 b9 28 95 23 98 a7 a2 24 69 24 65 21 49 22 e3 a4 55 1f fe 99 fe a5 b8 23 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer b7 15 cd 24 b9 a9 41 23 3b 1c c3 22 38 a8 f6 22 dd a8 01 a0 95 20 2e 96 48 2d 8a aa cf 1a 05 27 e7 2b 1f 0e ac 9c f2 ab 76 ab f8 25 ba aa 48 a9 c2 2b ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out', '4_layer_out_self_attn_out_dense' ],
        output: [ '4_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_plus_self_attn' ],
        output: [ '4_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '4_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '4_layer_out_plus_self_attn_mvn' ],
        output: [ '5_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 25 3b 07 3b 89 3a 72 3a bb 3b 2d 3b 0f 3b ff 3a ee 3a cf 3a 26 3a 0a 3a 54 3a 2d 3a f2 3a c7 3a 56 3a 55 3a 5c 3b 18 3b 3a 3b 9a 3a fd 3a 60 3b 5c 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 1b ac aa a2 14 2e 7f b3 6e 34 c8 ad 00 2e 55 30 47 a7 bd ae a2 ae 8c 2c 25 ac a7 2d c9 a9 e6 2d 1c a6 a9 1b f4 ac 42 2d 7e 32 4b b0 54 2d 88 36 a6 a9 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_attention_out' ],
        output: [ '5_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 38 a8 b7 2b 44 2a 6b 2c ff ac fb 9e c8 a6 cf 1e bc 29 bb a1 68 a5 1b a1 38 28 3f a5 7a 28 1f a9 07 25 2f a4 cc a0 40 a8 c4 24 fa ad d2 23 73 a7 f0 a5 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 65 ae 67 b4 57 b0 14 af b0 ae bd ad d9 ad d9 a9 22 aa 86 b0 6a af 43 af 1e a7 f0 b3 2b af 10 aa 36 b2 47 a9 c3 b0 9c aa ba ad 73 ae 30 ad 39 26 a3 ad ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_attention_out_ff1' ],
        output: [ '5_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '5_attention_out_gelu' ],
        output: [ '5_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 41 2c f8 a9 c6 2a 5a 28 15 a2 b9 a6 99 a8 0a 1b d9 a5 44 a5 b1 a6 98 a9 2b 24 6d 2a c0 26 df 23 80 21 c8 23 08 a2 56 1d 9d a4 8d 1e b6 a7 cf 9d 64 22 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 61 8d 00 2c b9 13 8a 28 53 2c 08 11 cc 2a 17 2c 7c a8 3c 1c c3 18 ee 22 a5 a9 87 2e 5c 24 eb 25 3a 25 46 28 0a 20 ad 20 4d 28 3d 1e c7 28 53 a9 11 23 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_attention_out', '5_attention_out_ff2' ],
        output: [ '5_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '5_attention_out_dense2_plus_input' ],
        output: [ '5_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '5_attention_out_dense2_plus_input_mvn' ],
        output: [ '5_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer e7 3a cd 3a 9e 3a 8a 3a 69 3a cb 3a c1 3a e1 3a fd 3a b8 3a 84 3a 3a 3a 8a 3a 87 3a dc 3a cb 3a 3b 3a b8 3a eb 3a 9c 3a ac 3a 75 3a ce 3a 6c 3a 99 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 28 9a eb aa 33 ac 73 2b 0b b2 ec 18 05 ad 26 af b7 a4 02 24 58 98 7b ad 45 9e 59 ac 2b 22 3a ac 2e aa d0 a8 6f 26 67 af e9 b0 e8 29 a7 ad d6 b1 0b a5 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out' ],
        output: [ '5_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3a a6 ec 29 61 a3 bf 2a 19 27 73 ac 44 13 99 ad c5 24 a5 25 f5 26 a5 a6 c8 a7 6c a8 3d 2c 51 2b a8 22 9d 1e 0c 2d db 29 c7 aa d4 29 40 a6 6b a5 79 1e ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 9d b1 7d 34 6d 31 40 af 62 31 75 af 37 b4 6e b0 f3 b4 4e b7 bd 2e 4a ad 33 ae 18 a5 b2 2f 3f 2d 38 af 20 2d 30 28 56 ac c8 99 85 33 bc 2f 82 30 0a ad ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query' ],
        output: [ '5_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '5_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query_heads_rank5' ],
        output: [ '5_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query_heads_rank4' ],
        output: [ '5_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out' ],
        output: [ '5_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 9c 2b 2a 26 d6 ac 6d 2f 39 a5 7d a0 0e ab 21 28 b5 28 79 25 06 28 cd 28 19 19 78 12 c1 1d 9f 2c 6a 26 d2 a8 91 20 6a 2d 80 25 11 9a 4d 22 56 ab c9 20 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 65 9c 4a 8a af 1c 90 9a 86 1c 7a 8d 20 9d 60 95 70 9d 35 24 73 86 ef 9a b5 1c 12 8c 8f 93 f3 98 51 9d 1c 9d c7 9b 12 92 ea 9c d9 97 ba 1d 93 9e 0c 1d ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_key' ],
        output: [ '5_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '5_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_key_heads_rank5' ],
        output: [ '5_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_key_heads_rank4' ],
        output: [ '5_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out' ],
        output: [ '5_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 58 a4 c1 1c c5 aa db 29 ab 20 ab a4 fd ac 23 ac 93 1f d4 ae 72 a9 b7 21 61 29 d7 ac 0d a2 dc 1a c1 26 a5 22 c6 27 8a 28 11 28 38 2c 39 22 77 26 77 2e ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer e9 a8 2c 25 68 97 3b a2 d7 95 3b 2a 64 23 82 99 5f 27 a7 25 33 19 f8 1c eb a1 5d 22 3f a2 27 a4 42 a5 5f 23 73 1a 38 1e b4 24 f8 9d 66 24 b1 29 85 a4 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_value' ],
        output: [ '5_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '5_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_value_heads_rank5' ],
        output: [ '5_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_value_heads_rank4' ],
        output: [ '5_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query_heads', '5_layer_out_key_heads' ],
        output: [ '5_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query_key_product' ],
        output: [ '5_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '5_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_query_key_product_masked' ],
        output: [ '5_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '5_layer_out_query_key_product_normalized',
          '5_layer_out_value_heads'
        ],
        output: [ '5_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_qkv' ],
        output: [ '5_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_qkv_T' ],
        output: [ '5_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '5_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_qkv_folded' ],
        output: [ '5_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_self_attn_out' ],
        output: [ '5_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 24 28 bc a6 41 21 01 9c a0 a2 96 a4 84 ab a0 24 08 98 d4 a6 9f a6 48 28 46 1a 7c ab 04 a6 52 a8 8c 24 8e 93 77 aa 6b 22 09 a9 59 24 f8 24 0a ab bc a0 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer ad 9c 63 ab 69 ab 8f a2 3a a7 23 2a 2c a8 6f a8 7e a5 39 aa 33 ab a8 9d 0c 2c a3 a9 62 27 39 a6 ed 25 36 a9 70 25 11 ad 4e ab 07 2c 26 ac bc a4 f7 27 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out', '5_layer_out_self_attn_out_dense' ],
        output: [ '5_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_plus_self_attn' ],
        output: [ '5_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '5_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '5_layer_out_plus_self_attn_mvn' ],
        output: [ '6_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 20 3b f5 3a 6f 3a 36 3a 68 3b e7 3a f0 3a df 3a c3 3a a2 3a 2e 3a be 39 37 3a 24 3a a6 3a ae 3a fe 39 47 3a 6a 3b bb 3a 00 3b 6d 3a de 3a 40 3b 23 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 39 ac 36 28 f8 30 db b0 b7 30 6b a9 0a 31 d9 29 fc ab d4 a4 77 ac c8 9a 52 b0 a3 31 17 28 a4 32 ea 2b 18 2d 16 a8 87 2c 80 31 8a ae bc 30 c0 34 00 2d ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_attention_out' ],
        output: [ '6_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer cb 29 0d aa bd a3 f6 a4 a2 a5 f1 a4 15 a5 16 a9 3f a5 af 2d b8 27 3a 25 7e a6 12 23 c0 26 de 25 5e 29 21 9e 6e 1f 4a ab a1 1e 5d 2c 37 a5 4f 24 af 1c ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 14 ae f5 b0 c0 ae 21 ae 78 b1 ce af 1a ae bc aa 6d ad da b3 ec ae 83 b0 44 b0 36 b0 c3 aa d7 a7 04 9a 36 aa 20 aa 8a a4 60 b1 47 b3 2a af 7d ae c5 22 ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_attention_out_ff1' ],
        output: [ '6_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '6_attention_out_gelu' ],
        output: [ '6_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 4b 99 cc 16 39 a4 c8 a3 a9 2b 4a 24 33 2c 4d a0 07 ae 23 2c 63 24 55 ab 20 23 8c 21 33 1c 4b 2b 98 a6 ef 22 a5 22 e6 ab 79 25 f2 ad d8 a5 f8 27 20 a8 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 0a a8 74 2b 83 28 6e 24 d9 2d 2d 28 92 2c 6d 30 0d a4 1a 1d 44 2b 5a 2c 5c ac a8 25 a7 9e 5c 28 de 2f 9d 29 1c a9 8a 1c 37 a5 4c a0 bc 28 3b 9c 44 27 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_attention_out', '6_attention_out_ff2' ],
        output: [ '6_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '6_attention_out_dense2_plus_input' ],
        output: [ '6_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '6_attention_out_dense2_plus_input_mvn' ],
        output: [ '6_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer f5 3a e8 3a 85 3a 9a 3a ac 3a d4 3a ad 3a e7 3a fc 3a c0 3a 9b 3a 8e 3a 73 3a 5f 3a 01 3b 86 3a 7d 3a a0 3a da 3a ba 3a de 3a 7f 3a d0 3a 97 3a a1 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3e a2 f6 a7 c4 ad 96 28 6f b0 23 a6 f0 ad a1 ad 28 18 ae a4 98 a1 e9 a6 a3 19 e1 ae 00 a8 d6 ad a2 ad 7e ac 06 9f a5 ae 8c b0 29 25 ff ae 41 b1 3b a9 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out' ],
        output: [ '6_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer c3 94 0a ad 0e a8 3f aa f4 9d a8 aa 54 24 88 09 26 26 da 9f cf 2c 52 ad 59 9f 03 27 30 28 a4 1a 1c a4 4d 20 14 29 ad 27 d9 91 fe a4 be ac 7b 2c 41 1d ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 44 10 d0 9c 0f 35 da 29 87 a1 34 27 47 ac ff a8 64 a8 d8 ab 38 9a 03 ac a2 a8 4b 23 d6 a0 2f ae 0e 34 3b a8 74 a6 48 34 20 29 3f a9 5b 26 60 b3 f8 21 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query' ],
        output: [ '6_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '6_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query_heads_rank5' ],
        output: [ '6_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query_heads_rank4' ],
        output: [ '6_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out' ],
        output: [ '6_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 51 27 03 9a 8b 2b ce 28 a2 28 12 25 73 ac 05 29 21 28 a2 aa 98 a9 1f a6 84 1a 96 14 e6 a1 91 27 b7 23 76 26 f1 22 f3 a8 81 23 bb 15 ed 24 99 a8 f6 9c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 63 92 63 1f 74 15 2f 1b 6e 10 e7 21 0b 9e e7 1e 66 96 3d 97 0e 91 55 9a 5a 22 88 1d e3 9a d6 98 1a 1a be 14 34 20 ac 24 31 9d 76 18 7b 0f 02 95 25 98 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_key' ],
        output: [ '6_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '6_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_key_heads_rank5' ],
        output: [ '6_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_key_heads_rank4' ],
        output: [ '6_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out' ],
        output: [ '6_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer d2 a0 60 28 fa a8 ae 2d ec aa da a7 4d a8 39 25 8b 29 67 a4 5a 29 7f 9d c3 22 1c 28 7c 1d 93 9a 5a a8 29 23 6e 25 96 1f fd 24 f8 a3 b4 a6 1f 9c b0 2b ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer e6 a6 33 24 c8 20 ed a8 19 20 0d a2 88 a4 af 9c 78 1b 34 9c 72 a0 d9 a8 14 19 b5 a8 fe 22 51 a6 69 a8 ee 29 6d a0 28 28 cf 23 93 a4 07 20 b7 22 24 ac ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_value' ],
        output: [ '6_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '6_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_value_heads_rank5' ],
        output: [ '6_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_value_heads_rank4' ],
        output: [ '6_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query_heads', '6_layer_out_key_heads' ],
        output: [ '6_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query_key_product' ],
        output: [ '6_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '6_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_query_key_product_masked' ],
        output: [ '6_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '6_layer_out_query_key_product_normalized',
          '6_layer_out_value_heads'
        ],
        output: [ '6_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_qkv' ],
        output: [ '6_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_qkv_T' ],
        output: [ '6_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '6_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_qkv_folded' ],
        output: [ '6_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_self_attn_out' ],
        output: [ '6_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer ea aa d3 26 a2 a2 83 9e fd a8 7b a4 17 24 d2 22 1c a6 6e a8 e8 a7 08 1e a0 21 e7 25 bf a6 e0 9f c8 a4 2e 27 56 a0 d9 a4 4b a7 66 a4 60 26 f2 ab de 2c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 64 1b 47 ab 81 ab c1 a8 4b 28 16 a3 ff a6 bb 28 a8 a4 42 aa 90 98 eb 28 ae 21 2f 1e 0b 21 c3 aa 1b a5 b8 a7 08 28 e2 a8 58 ac 1b 26 7e aa 34 a8 a7 25 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out', '6_layer_out_self_attn_out_dense' ],
        output: [ '6_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_plus_self_attn' ],
        output: [ '6_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '6_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '6_layer_out_plus_self_attn_mvn' ],
        output: [ '7_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 03 3b f2 3a 57 3a 14 3a 65 3b c6 3a d6 3a db 3a c0 3a 94 3a f0 39 75 39 3c 3a 0c 3a b8 3a c6 3a 3e 3a 46 3a 52 3b ae 3a da 3a 53 3a a6 3a 13 3b 48 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 39 a7 eb 29 1e 2b 76 aa 4f 31 d0 a6 c5 31 51 2e d9 a4 b2 ae 1e 29 21 2d 1f b1 5f 24 91 20 c6 34 1f 31 c6 2e 12 ad 07 23 09 32 a1 af 0d 0c 83 34 3f 2f ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_attention_out' ],
        output: [ '7_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 92 28 d0 9e 94 a3 6b 02 e5 2c 50 a0 d2 28 fc a6 1e a7 7b a9 1d 24 90 2d 3f a4 84 20 4a 2b e1 1f 88 a5 a1 22 b8 a6 87 28 d7 26 ba aa 8e 9d 46 a4 ac a9 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer e6 af 8f ad eb ab e9 ac b2 ad 02 b1 60 ac 7c ae 85 ab e9 ae 69 b1 04 ad a4 a8 bc ae e0 b2 22 b1 d8 af 9d ab f1 b3 11 ac 1e 9f 40 ad 2f ae 4d b0 01 ae ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_attention_out_ff1' ],
        output: [ '7_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '7_attention_out_gelu' ],
        output: [ '7_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer f7 a5 c5 af 11 1f a4 24 50 27 9a a4 83 22 b8 20 bc 1c 65 22 5d 9f 83 a2 18 1f 6b ad 6e a4 4e 27 7a 28 95 25 2d 2b 15 a9 cb a3 45 ab 78 a4 2f a9 af a1 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 8a a9 3f 29 ec 2b 3c aa 56 28 4a aa 02 2b 80 2d 5d ad 27 a5 f3 22 23 2f 1e ae cf 98 54 a8 78 28 4c 31 c9 2c 50 a8 26 2a 11 24 d5 2b 24 a6 a0 a5 a9 aa ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_attention_out', '7_attention_out_ff2' ],
        output: [ '7_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '7_attention_out_dense2_plus_input' ],
        output: [ '7_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '7_attention_out_dense2_plus_input_mvn' ],
        output: [ '7_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer b9 3a 83 3a 7a 3a 7d 3a 48 3a 96 3a 4e 3a b7 3a b9 3a 87 3a 4e 3a 4d 3a 7c 3a 60 3a 7b 3a 3b 3a 51 3a 64 3a 8d 3a 8a 3a a4 3a 6f 3a a2 3a 4b 3a 58 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 0a a8 37 a9 77 aa c6 a4 2d b1 1d a9 36 ae d7 ac a7 a7 19 a7 81 a9 97 ac c7 26 aa ac 4f a9 ca ae 64 ae de ac 4d 20 69 ac 39 b0 a5 24 41 ae 14 b2 7e ad ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out' ],
        output: [ '7_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer ca 29 e0 2b 8b 25 09 2e f3 25 ea a9 69 29 d9 ab 76 2c 33 27 58 a1 7b aa de 2a 67 a6 18 a4 19 2e 12 af 3d a5 3c a7 28 ab e8 20 92 2b 58 a2 18 2a 88 9c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 07 b4 9a b3 a8 26 61 21 d8 31 0a af dd 36 71 30 e5 37 58 b2 fd aa b5 2a 56 af 81 af f9 b1 9f ad 62 af 25 ac a6 2c ea 39 b2 2f 68 b6 fb 2d c7 30 64 a0 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query' ],
        output: [ '7_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '7_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query_heads_rank5' ],
        output: [ '7_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query_heads_rank4' ],
        output: [ '7_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out' ],
        output: [ '7_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer bc a4 50 96 1e 2a 6b 23 fa 2a fe a8 67 9b bf 1d 18 ab c7 26 33 a4 00 25 37 28 f3 a8 ac 27 3c a6 7a 1e fb 24 9b ab a8 1c cd 24 83 ab 4d 28 be a6 65 a9 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 0a 04 1d 1b 3e 97 46 9e 11 8f 2a a1 bd 9a 86 a4 9b 9d a8 9c 7d 91 01 1c fe 12 d5 1e 32 1e f1 17 77 1d a5 1f 3b 1d b8 a0 b3 96 e0 20 b9 0e 31 1e 65 21 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_key' ],
        output: [ '7_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '7_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_key_heads_rank5' ],
        output: [ '7_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_key_heads_rank4' ],
        output: [ '7_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out' ],
        output: [ '7_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 59 9d 20 a6 66 a6 f8 23 bc 24 3b a9 8c a8 3a 2b 58 9d 33 a5 51 a1 ed 9e 81 a5 76 a2 e7 1d 00 a1 dc 29 d9 29 4c 9c ff 93 5d a0 aa 1a 99 21 93 a3 3a a7 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 6f 26 79 1f 8f a8 27 1a f4 a5 7f 22 78 a0 50 a6 30 28 a1 28 2e 18 65 1f f0 25 99 25 5f 25 8a a0 c0 9d 41 a8 53 aa 3c a6 8d 21 51 a3 a9 a4 77 26 2c a7 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_value' ],
        output: [ '7_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '7_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_value_heads_rank5' ],
        output: [ '7_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_value_heads_rank4' ],
        output: [ '7_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query_heads', '7_layer_out_key_heads' ],
        output: [ '7_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query_key_product' ],
        output: [ '7_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '7_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_query_key_product_masked' ],
        output: [ '7_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '7_layer_out_query_key_product_normalized',
          '7_layer_out_value_heads'
        ],
        output: [ '7_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_qkv' ],
        output: [ '7_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_qkv_T' ],
        output: [ '7_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '7_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_qkv_folded' ],
        output: [ '7_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_self_attn_out' ],
        output: [ '7_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 04 29 e7 a5 03 24 58 22 3c 9a 63 a6 c8 2c 05 a7 84 26 fa 29 af a4 5d a7 c0 a8 d5 2b 81 99 d3 a6 c5 24 4c 27 28 a4 e6 29 5c 24 c8 26 1c 27 79 9c af 27 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 39 22 48 ab 03 ac 29 1e b8 27 80 ac 41 a7 ea 1d f8 a4 c8 a8 47 28 b1 15 11 28 c2 a7 fd 25 58 ae 38 ac ef ac 4d 26 70 a7 08 ad 48 25 85 ac 9d aa 9e 20 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out', '7_layer_out_self_attn_out_dense' ],
        output: [ '7_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_plus_self_attn' ],
        output: [ '7_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '7_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '7_layer_out_plus_self_attn_mvn' ],
        output: [ '8_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 26 3b 1b 3b 7d 3a 41 3a 3d 3b da 3a 26 3b eb 3a e4 3a d8 3a 0a 3a 42 3a 3c 3a 45 3a e0 3a ec 3a 7d 3a 70 3a 62 3b a7 3a e4 3a 5e 3a cb 3a ed 3a 4e 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 17 ac b7 29 84 2d 8f b0 35 aa 7a 9f 06 34 e0 2d ae ad b3 b1 63 28 33 2f b9 b0 40 26 7a ad d1 34 6b 32 24 2e 69 ac 58 91 a7 31 7a a1 5c ab 7c 2e 82 29 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_attention_out' ],
        output: [ '8_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 15 a5 d2 28 c0 a8 eb 9e 7a ae 9e 97 35 a8 6e a9 47 a7 a1 a9 5f 2b a0 a5 cb 29 99 a6 0a ac ce a6 44 29 51 a9 70 ac de a6 78 29 4b 24 e3 a8 3d ab dc 10 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 0d ad 49 b1 e1 ae 91 ae 21 b0 1e a2 c7 af 66 23 17 b0 d7 af f6 b0 8e ae db af 66 ae 9d b0 aa ad 02 ae 91 ac e6 af 59 b0 52 af cc a9 73 ac c7 af 6b af ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_attention_out_ff1' ],
        output: [ '8_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '8_attention_out_gelu' ],
        output: [ '8_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 25 aa b4 26 32 a7 69 ab 6b a0 23 2d 49 a6 8d 9f e0 a6 ab 28 b1 29 df ab 17 a6 9d 28 28 a7 78 9e 87 29 c8 25 ca a6 f9 a4 08 2b 04 0b 59 26 27 a4 bb a9 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 05 ae c8 a9 47 2b d3 ad fd 2f 9d 2c 2c 2a 45 30 01 ad 50 a3 2f a6 cc 29 93 ae 5f 2e 68 a7 bd 2e ee 33 21 2d 1d a1 4b 28 17 a1 ab 29 d2 25 70 25 42 ac ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_attention_out', '8_attention_out_ff2' ],
        output: [ '8_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '8_attention_out_dense2_plus_input' ],
        output: [ '8_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '8_attention_out_dense2_plus_input_mvn' ],
        output: [ '8_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer c6 3a bb 3a 99 3a c4 3a 91 3a bd 3a 65 3a aa 3a d0 3a 9e 3a a4 3a 7b 3a 96 3a 87 3a b6 3a 54 3a 95 3a a9 3a 9a 3a c4 3a cd 3a b7 3a d0 3a 8c 3a 91 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 1e ac 79 ac 1f ab 7c 90 af ae ad a5 33 b0 1f ad 90 9e d0 22 0e ad 8c af dd 28 75 ab fc a6 ab b0 22 ad bd ad 86 a4 78 ac 35 b0 2f a5 8b ac 84 b1 13 ac ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out' ],
        output: [ '8_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 16 2d f5 1e 85 9d a6 aa c0 29 da 1e c6 26 e8 1f d4 27 99 ad 48 2b 6c 2c 9e a2 d6 a8 75 25 53 29 e6 28 3c 28 1f a6 69 22 97 a4 86 a5 fb 29 8f 29 0c a8 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer a4 b0 85 36 40 b5 fd a6 2f b4 36 a0 84 34 ad 34 a5 ad 5e 36 6b b7 e9 33 96 11 43 31 5c b1 25 2f 42 b1 4e b2 9b b7 f7 23 29 b5 7f b2 c2 b4 0c ba f2 b3 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query' ],
        output: [ '8_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '8_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query_heads_rank5' ],
        output: [ '8_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query_heads_rank4' ],
        output: [ '8_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out' ],
        output: [ '8_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer c5 a3 12 1f 91 aa f9 25 ab 20 17 ab d7 a7 9b 28 5d a6 d7 28 bb a0 df aa d2 24 36 ad b3 29 54 27 5e 28 a3 29 f1 9f 1d 21 2b 27 55 ab 60 2d 0a ad 7e 9b ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3d 9d 43 20 ce 1b bc 20 37 9c 13 a0 59 1b 63 1d be 9a b8 1b c2 a3 2f 8b f5 10 32 21 c8 9b 3e 1a b5 9d 15 9e 93 1a 8f 9b b0 9b d9 a0 8e a0 ab 20 21 93 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_key' ],
        output: [ '8_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '8_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_key_heads_rank5' ],
        output: [ '8_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_key_heads_rank4' ],
        output: [ '8_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out' ],
        output: [ '8_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 1e a6 24 24 b3 29 74 a4 7e a5 67 97 50 29 85 a7 66 a7 70 1b f2 28 83 24 05 2c fa 9e 5a 27 e0 24 a0 29 52 a8 90 a7 c9 22 56 a9 88 20 72 a8 b4 22 ea a5 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 6a a9 4e a8 13 1f 27 15 d1 2a 94 a7 e8 a9 17 1e 23 14 58 1f be a0 57 24 e0 a1 4e a5 49 a4 2c a1 e3 23 ac a8 67 a9 75 a5 f0 09 f2 a9 56 ad 18 25 70 ab ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_value' ],
        output: [ '8_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '8_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_value_heads_rank5' ],
        output: [ '8_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_value_heads_rank4' ],
        output: [ '8_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query_heads', '8_layer_out_key_heads' ],
        output: [ '8_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query_key_product' ],
        output: [ '8_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '8_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_query_key_product_masked' ],
        output: [ '8_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '8_layer_out_query_key_product_normalized',
          '8_layer_out_value_heads'
        ],
        output: [ '8_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_qkv' ],
        output: [ '8_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_qkv_T' ],
        output: [ '8_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '8_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_qkv_folded' ],
        output: [ '8_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_self_attn_out' ],
        output: [ '8_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 29 a2 ae 26 ae a0 ad a3 bc 2a d1 24 5d 22 a5 25 76 aa b5 9a 55 2d 5b 1f 64 a7 55 23 67 9f a7 a8 3f a0 18 2e 57 25 e2 26 12 a3 5e a8 c2 1b 76 20 39 21 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer a7 25 4a 2a 7a ac 23 a0 22 22 ce aa 46 a8 41 ab 9c 23 0d a9 57 25 96 a3 50 2b 38 a9 68 ab 07 a0 bd a5 87 03 28 ac c0 29 48 ac ef 22 b7 aa 71 22 43 27 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out', '8_layer_out_self_attn_out_dense' ],
        output: [ '8_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_plus_self_attn' ],
        output: [ '8_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '8_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '8_layer_out_plus_self_attn_mvn' ],
        output: [ '9_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer d6 3a 98 3a 58 3a 1c 3a 25 3b a6 3a bf 3a e1 3a b1 3a a4 3a 15 3a 10 3a 39 3a 60 3a 81 3a b3 3a 8e 3a 1f 3a 2e 3b 72 3a cc 3a 3c 3a 78 3a be 3a 22 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer f8 ad 1f ac 3a 2c 46 ae 7b 2d ea 2d 34 31 95 31 31 b0 6e b1 dc 22 8e 24 77 b2 ba 2c e3 1a ee 33 a8 32 50 2c c1 ad ea a9 57 32 11 a5 53 a9 fa 2d 22 9a ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_attention_out' ],
        output: [ '9_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer b7 2b 9d a0 4b 1c 67 2e 21 aa b0 18 da 17 15 28 3d a0 af 1b b4 28 02 1e 66 2c 57 a8 38 a8 0b a5 3a 9d 8c 24 f1 1c 67 25 5e ab 1c 9d 12 a5 85 9f a1 a8 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 18 af ad 94 10 ae 8e ae ae b0 a2 af a1 af fd af fe aa 3b af d7 b0 37 af 0b af 4b ac 26 b0 78 ad 09 ab dc ac 7f b0 f4 ad f7 ab 8b ac ec af a3 ad 42 ae ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_attention_out_ff1' ],
        output: [ '9_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '9_attention_out_gelu' ],
        output: [ '9_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer af 2a 11 20 4f 23 6f 2c d5 23 b8 1f 19 a7 09 24 6a 26 4c a1 2c a5 f7 2c e6 28 39 2c f6 29 7e 0c c2 a7 11 1d 89 a5 75 a1 b6 aa 81 26 5e 19 9f 2a 47 a6 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer a9 ad d8 a5 a6 a7 20 ad 29 2e 89 2b 36 2c 56 2c 57 ab 72 ac 04 25 53 2a 8c ab d9 2f 41 a4 c6 30 9b 31 01 25 16 20 eb a4 34 2c a5 24 9a 29 36 26 f0 a8 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_attention_out', '9_attention_out_ff2' ],
        output: [ '9_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '9_attention_out_dense2_plus_input' ],
        output: [ '9_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '9_attention_out_dense2_plus_input_mvn' ],
        output: [ '9_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 9f 3a ab 3a 3f 3a 79 3a 44 3a 8d 3a 35 3a 35 3a 98 3a 61 3a 34 3a 45 3a 55 3a 6c 3a 9a 3a 46 3a 3d 3a 51 3a 88 3a a4 3a 4d 3a 53 3a ab 3a 70 3a 7c 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 4d a9 75 a6 ec ac b6 9f 70 ae 55 ac 61 af 50 ad 9a 98 12 20 42 ac 15 ad a8 26 23 ac 16 aa a6 ae 94 ae 69 ac a9 23 c7 a9 45 af 72 a6 bd ad 1d b1 76 ab ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out' ],
        output: [ '9_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer f4 14 df 28 09 9f 48 a6 44 26 6e ab 69 aa 3c 25 2e aa aa 23 3d ac ba 1e 8d ab 95 2d aa 22 ba ad f9 a7 c9 19 8b a6 92 ac c1 25 74 ad 78 ab a4 a4 d6 ab ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer d0 a8 24 a0 e6 a8 c7 0c f5 aa d7 ab 0c a3 59 2c fd a4 0f a9 82 9d 03 25 bc a4 00 2d 33 1c 21 a9 04 2c b6 24 6f 28 7d a7 60 2f 9b 2b bc 29 03 22 e8 2c ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query' ],
        output: [ '9_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '9_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query_heads_rank5' ],
        output: [ '9_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query_heads_rank4' ],
        output: [ '9_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out' ],
        output: [ '9_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 2e aa 3b a8 98 24 1c a5 af a6 a5 25 b3 a8 41 1d 03 ab 5b 28 48 1c fe 29 a7 9b 78 2b bc a2 9c 2c 3e 2b 7c ac 05 a1 2e 9f 62 ac dc 24 ad 2d 9b 9c 8e a6 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 1c 9f 67 20 d9 1e 68 9d d6 89 1f 1d 3b 1c 70 17 f0 93 0c 15 81 0b a2 8c a7 90 77 99 c8 9c 79 0f a7 a4 f7 1c 96 9c 83 9d 88 1a 16 18 10 1d fc 1f fb 19 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_key' ],
        output: [ '9_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '9_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_key_heads_rank5' ],
        output: [ '9_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_key_heads_rank4' ],
        output: [ '9_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out' ],
        output: [ '9_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer a6 25 2f 26 ef ac 59 a3 2d a2 c5 a2 cd 2a 3c a2 e2 1e 3b aa d5 2a 89 28 21 18 ca 28 0d 18 2f a0 c3 a5 c6 ab 5b a8 07 1e db a4 52 24 6e 1f 6d a5 e8 2d ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 04 a4 68 a3 be 9d d3 9c 58 1b 14 27 dc 9d 78 21 2d 24 fb a6 da a4 f5 a8 04 10 8f 1a 40 19 a6 87 38 9d ba a2 fa a2 94 9f 90 a3 35 1f e1 9b e5 9c b4 9d ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_value' ],
        output: [ '9_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '9_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_value_heads_rank5' ],
        output: [ '9_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_value_heads_rank4' ],
        output: [ '9_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query_heads', '9_layer_out_key_heads' ],
        output: [ '9_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query_key_product' ],
        output: [ '9_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '9_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_query_key_product_masked' ],
        output: [ '9_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '9_layer_out_query_key_product_normalized',
          '9_layer_out_value_heads'
        ],
        output: [ '9_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_qkv' ],
        output: [ '9_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_qkv_T' ],
        output: [ '9_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '9_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_qkv_folded' ],
        output: [ '9_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_self_attn_out' ],
        output: [ '9_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 07 ab f3 a8 22 18 04 22 bc 29 39 a4 f4 a3 ba a9 78 1c 76 1b b9 2b 8f aa c4 21 5e 1b 30 98 ba 25 52 27 26 25 6f 9f 7e 29 df 2d 47 a5 9e 2a 9d a5 0c 2c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 02 25 05 21 0c 2b 6a 28 e1 1e 7e 23 1d 25 f5 ab 3b 27 30 2c ee a7 50 ac 4e ac 39 a3 29 ab 94 a5 6a 2b 6d 2c b0 20 08 94 16 a5 37 08 60 a5 05 1c 5c 28 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out', '9_layer_out_self_attn_out_dense' ],
        output: [ '9_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_plus_self_attn' ],
        output: [ '9_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '9_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '9_layer_out_plus_self_attn_mvn' ],
        output: [ '10_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer d4 3a 83 3a b3 3a 3b 3a 0d 3b a2 3a 96 3a f9 3a ab 3a a9 3a 7c 3a 6f 3a 54 3a 7b 3a 8b 3a b9 3a 9a 3a 66 3a 06 3b 90 3a c1 3a 82 3a 67 3a 81 3a 63 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 18 af 47 aa 4d 27 d2 ab 8b 30 74 25 92 1f ba 30 34 af c0 a5 cf 1e 63 29 3c b0 a8 2c 6a ac 77 30 fe 2a 7c 9f 30 a4 0b a8 9e 2c 50 a5 ab 9a 6c 2b b1 ad ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_attention_out' ],
        output: [ '10_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 99 ac 71 a6 ea 25 f5 a3 e0 a5 e0 19 e1 93 50 9b b7 ae 03 29 18 28 0e 28 ec 9a 2b 20 6a 93 6d a9 80 28 d0 25 ea 25 59 10 90 ab fa 14 2d 9d 2f a4 05 18 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 5e b0 17 ac 8c b0 ad b0 70 ae c2 28 b4 ae 59 af 51 ad 95 a8 f6 ae 19 b0 cc af 3f af 86 af 0a b0 0e b0 59 ad d0 ad 34 b0 27 af f9 ac ab ac 63 ad 04 ae ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_attention_out_ff1' ],
        output: [ '10_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '10_attention_out_gelu' ],
        output: [ '10_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3a 27 b1 a7 9a a2 b9 28 e6 95 fe 9a 69 a4 4f a2 77 9e 0a 2c 47 1e 5c 2b c5 28 9f 28 96 28 a9 20 98 2b 58 1e 79 a2 4d 28 a1 a7 8d a3 1b 19 95 ac 2e 25 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 30 a4 d3 2e 59 a5 cb a7 8d 2e ba 2f ae 2d 77 2d 8e ad 55 af de ac 67 24 af a9 6f 2d 01 2d 2e 34 e7 30 b5 2e 30 13 ce a9 a1 2e 5e ac 5f 2a 83 31 a3 2a ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_attention_out', '10_attention_out_ff2' ],
        output: [ '10_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '10_attention_out_dense2_plus_input' ],
        output: [ '10_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '10_attention_out_dense2_plus_input_mvn' ],
        output: [ '10_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer a1 3a e2 3a 59 3a 7e 3a a1 3a 9f 3a 7b 3a 60 3a b6 3a 7f 3a 76 3a 5b 3a 51 3a 7d 3a b6 3a 54 3a 68 3a 92 3a ea 3a bf 3a a2 3a 7c 3a bf 3a 9a 3a 89 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer fe 22 e9 a6 5b ad a6 9f 52 ad a9 ad c2 aa 78 ae e9 9f 78 a5 69 a6 51 ab 57 24 57 ae 9d a5 3c b0 64 ae 23 ac 6c a5 9d ab eb ae 6f a8 89 a3 fc ad a5 ab ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out' ],
        output: [ '10_layer_out_query' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 4f a9 90 97 32 a9 e5 a3 33 2a b5 27 e2 29 2d 28 83 a0 ae 25 aa 24 df 28 30 ac 43 a2 a1 a7 d7 a5 38 ae e4 ab 02 a9 6e 2b 47 ab 24 a9 4f ad df 2e 2f 17 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 76 2a b7 2f 0a 33 b1 2d 98 b2 4b 33 89 b5 0f 2c b2 b6 5d 2f 93 34 fa ae c3 32 33 2c c2 ad d9 ab 50 2f 23 b2 fe a1 fd b3 a9 2c a1 b2 cc b1 9e a4 52 2b ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query' ],
        output: [ '10_layer_out_query_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '10_layer_out_query_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query_heads_rank5' ],
        output: [ '10_layer_out_query_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query_heads_rank4' ],
        output: [ '10_layer_out_query_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out' ],
        output: [ '10_layer_out_key' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_key',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 43 27 c1 a1 a5 a3 14 27 22 a4 f4 a5 b1 a4 2a 2e 58 ac 22 a9 f2 17 6b 18 99 a9 03 ab 96 2a 4f aa 28 28 e8 99 29 a9 9e a0 50 a0 62 aa 1c 20 49 23 4f 2c ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 60 92 69 91 ca 86 e3 14 63 97 a1 99 78 92 4b 19 c7 a5 82 99 db 18 b5 1a 0a 17 03 17 b8 0f 70 9c 52 15 c9 1b 82 03 02 99 e5 99 2a 9b 1a 9b 69 19 19 9e ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_key' ],
        output: [ '10_layer_out_key_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '10_layer_out_key_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_key_heads_rank5' ],
        output: [ '10_layer_out_key_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_key_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_key_heads_rank4' ],
        output: [ '10_layer_out_key_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_key_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out' ],
        output: [ '10_layer_out_value' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_value',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 47 28 fd aa 58 1f 6a 28 bf ac 9c 20 54 ab 7c 98 37 21 8d a0 b5 9a 1e 27 e8 28 7f aa 85 ac f1 a8 78 a5 22 22 80 29 3f aa 8a ab 4d 2b 30 25 a9 aa ae 29 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 21 23 8c 9d c5 1b a7 90 0f 9f 46 1f db a1 47 a0 14 96 ad 9d 38 98 2f 1c 3f 9e a0 1d 44 22 0f 94 d6 1e b7 21 72 1f 51 a1 cd 20 1d 9f f8 a2 13 9d da 20 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_value' ],
        output: [ '10_layer_out_value_heads_rank5' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 12, high: 0, unsigned: false },
              Long { low: 64, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false }
            ],
            rank: 5
          }
        ],
        name: '10_layer_out_value_heads_rank5',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 12, high: 0, unsigned: false },
            Long { low: 64, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_value_heads_rank5' ],
        output: [ '10_layer_out_value_heads_rank4' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_value_heads_rank4',
        squeeze: SqueezeLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_value_heads_rank4' ],
        output: [ '10_layer_out_value_heads' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_value_heads',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query_heads', '10_layer_out_key_heads' ],
        output: [ '10_layer_out_query_key_product' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query_key_product',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query_key_product' ],
        output: [ '10_layer_out_query_key_product_scaled' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query_key_product_scaled',
        multiply: MultiplyLayerParams { alpha: 0.125 }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query_key_product_scaled', 'attention_mask' ],
        output: [ '10_layer_out_query_key_product_masked' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query_key_product_masked',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_query_key_product_masked' ],
        output: [ '10_layer_out_query_key_product_normalized' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_query_key_product_normalized',
        softmaxND: SoftmaxNDLayerParams {
          axis: Long { low: -1, high: -1, unsigned: false }
        }
      },
      NeuralNetworkLayer {
        input: [
          '10_layer_out_query_key_product_normalized',
          '10_layer_out_value_heads'
        ],
        output: [ '10_layer_out_qkv' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_qkv',
        batchedMatmul: BatchedMatMulLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_qkv' ],
        output: [ '10_layer_out_qkv_T' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_qkv_T',
        transpose: TransposeLayerParams {
          axes: [
            Long { low: 0, high: 0, unsigned: true },
            Long { low: 2, high: 0, unsigned: true },
            Long { low: 1, high: 0, unsigned: true },
            Long { low: 3, high: 0, unsigned: true }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_qkv_T' ],
        output: [ '10_layer_out_qkv_folded' ],
        inputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        outputTensor: [
          Tensor {
            dimValue: [
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 0, high: 0, unsigned: false },
              Long { low: 768, high: 0, unsigned: false },
              Long { low: 1, high: 0, unsigned: false }
            ],
            rank: 4
          }
        ],
        name: '10_layer_out_qkv_folded',
        rankPreservingReshape: RankPreservingReshapeLayerParams {
          targetShape: [
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 0, high: 0, unsigned: false },
            Long { low: 768, high: 0, unsigned: false },
            Long { low: 1, high: 0, unsigned: false }
          ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_qkv_folded' ],
        output: [ '10_layer_out_self_attn_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_self_attn_out',
        expandDims: ExpandDimsLayerParams {
          axes: [ Long { low: -1, high: -1, unsigned: false } ]
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_self_attn_out' ],
        output: [ '10_layer_out_self_attn_out_dense' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_self_attn_out_dense',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 2d 25 4c aa ea 29 18 24 16 1d 7c 9f 25 2c 62 ab 6b 27 83 23 8f 9d 4b a7 12 a7 16 2d 29 ab cb a4 e5 28 91 ac 66 23 35 20 16 29 91 a0 30 2b 89 26 47 a8 ... 1179598 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer fb 26 d8 1c 2a 9d 26 26 6e a5 7b ab f2 a0 09 a5 d3 29 34 2a b6 2c 27 a6 13 27 04 a7 76 21 da a2 61 1f d8 a8 e9 9d d3 9d dd 28 b3 25 97 a8 a3 1a 2d 21 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out', '10_layer_out_self_attn_out_dense' ],
        output: [ '10_layer_out_plus_self_attn' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_plus_self_attn',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_plus_self_attn' ],
        output: [ '10_layer_out_plus_self_attn_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '10_layer_out_plus_self_attn_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '10_layer_out_plus_self_attn_mvn' ],
        output: [ '11_attention_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_attention_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer d7 3a 69 3a d5 3a 85 3a c1 3a 90 3a d8 3a ca 3a cc 3a e4 3a 95 3a e3 3a b9 3a 80 3a a1 3a 41 3b e8 3a b4 3a 98 3a e2 3a b6 3a b2 3a 9d 3a b0 3a 9e 3a ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 8f a7 5b 22 f3 a9 2b a7 c0 2c 75 2a b0 20 48 a5 46 af 3c af 86 ad 1b 27 8a af a0 24 5d a7 7f 28 de 2a b4 29 41 9f d0 ac 3b a6 17 a8 6a 2c 93 28 91 28 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '11_attention_out' ],
        output: [ '11_attention_out_ff1' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_attention_out_ff1',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 3072, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 97 2b 20 2b 0d a5 90 a5 ba 2a 19 1a ae 25 87 2a d7 a6 26 1d d8 2d a2 2a ad 14 af a8 52 2c 41 24 7c aa fc a4 ef ac 92 a0 ee a6 83 28 b8 11 eb a9 a4 2d ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 3d ae 1c ac 38 aa 75 ad 07 ae af ac d2 ac ec ac 31 ac a4 aa f9 b0 5e ad 9f ac eb ad da b1 42 a9 3f a7 43 ac a3 aa a7 ac 44 ac 02 ac 00 ae d5 ad 5e ad ... 6094 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '11_attention_out_ff1' ],
        output: [ '11_attention_out_gelu' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_attention_out_gelu',
        gelu: GeluLayerParams { mode: 2 }
      },
      NeuralNetworkLayer {
        input: [ '11_attention_out_gelu' ],
        output: [ '11_attention_out_ff2' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_attention_out_ff2',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 3072, high: 0, unsigned: true },
          outputChannels: Long { low: 768, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 93 a4 bd 2a 6b a1 69 a6 76 24 54 a3 95 28 73 a8 e3 ab 8e a2 79 a3 56 21 46 a3 8f 28 d1 a1 b2 24 cb 18 a0 9f f7 9b 60 aa 6e a4 c7 aa c8 a4 99 a1 27 21 ... 4718542 more bytes>
          },
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 16 ac 03 2a 6f 23 bc 1b 90 2a f1 29 1d 25 d7 aa c8 a6 15 ae 69 ae f4 2a c0 ac 77 2d af 2b ab 2f 6e 1d c6 1b 95 27 ce a7 8a 26 b7 9f 24 1a c1 24 49 27 ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '11_attention_out', '11_attention_out_ff2' ],
        output: [ '11_attention_out_dense2_plus_input' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_attention_out_dense2_plus_input',
        add: AddLayerParams {}
      },
      NeuralNetworkLayer {
        input: [ '11_attention_out_dense2_plus_input' ],
        output: [ '11_attention_out_dense2_plus_input_mvn' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_attention_out_dense2_plus_input_mvn',
        mvn: MeanVarianceNormalizeLayerParams {
          acrossChannels: true,
          normalizeVariance: true,
          epsilon: 9.999999960041972e-13
        }
      },
      NeuralNetworkLayer {
        input: [ '11_attention_out_dense2_plus_input_mvn' ],
        output: [ '11_layer_out' ],
        inputTensor: [],
        outputTensor: [],
        name: '11_layer_out',
        scale: ScaleLayerParams {
          shapeScale: [ Long { low: 768, high: 0, unsigned: true } ],
          shapeBias: [ Long { low: 768, high: 0, unsigned: true } ],
          scale: WeightParams {
            floatValue: [],
            float16Value: <Buffer 20 39 0d 39 54 39 ec 38 1d 39 1a 39 42 39 17 39 07 39 05 39 04 39 3b 39 e1 38 39 39 0e 39 fd 38 42 39 d1 38 28 39 0f 39 0d 39 0d 39 fe 38 0a 39 11 39 ... 1486 more bytes>
          },
          hasBias: true,
          bias: WeightParams {
            floatValue: [],
            float16Value: <Buffer 7e 9e 17 a5 ec 29 11 24 74 a9 cc a2 af a0 4a a8 9d 26 aa a8 d4 ab ca 27 65 24 3e 22 d0 9a 37 aa 44 10 ea ad 4e 21 1c 22 84 ab 06 9f 78 1c cb a9 a7 1d ... 1486 more bytes>
          }
        }
      },
      NeuralNetworkLayer {
        input: [ '11_layer_out' ],
        output: [ 'start_logits' ],
        inputTensor: [],
        outputTensor: [],
        name: 'start_logits',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 1, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer 66 a0 e6 22 4a a5 9b 23 7f a3 a9 a4 ec 24 d6 26 96 a1 8c 0f 58 24 13 25 a7 a4 ee 12 b7 a5 87 a8 2d 14 4e 22 f5 a7 9b a4 85 23 06 a8 a9 20 15 99 00 a7 ... 1486 more bytes>
          },
          bias: WeightParams { floatValue: [], float16Value: <Buffer 5a 11> }
        }
      },
      NeuralNetworkLayer {
        input: [ '11_layer_out' ],
        output: [ 'end_logits' ],
        inputTensor: [],
        outputTensor: [],
        name: 'end_logits',
        innerProduct: InnerProductLayerParams {
          inputChannels: Long { low: 768, high: 0, unsigned: true },
          outputChannels: Long { low: 1, high: 0, unsigned: true },
          hasBias: true,
          weights: WeightParams {
            floatValue: [],
            float16Value: <Buffer da 22 fe 16 04 1e 7f 27 c8 a4 49 25 bf a7 1f 1e f1 a4 82 11 80 a3 3c a1 d4 a7 52 24 3a a3 40 ab df a7 26 22 b1 9a cd 25 72 28 09 a7 1b a7 23 9a 08 ac ... 1486 more bytes>
          },
          bias: WeightParams { floatValue: [], float16Value: <Buffer d2 11> }
        }
      }
    ],
    preprocessing: [],
    arrayInputShapeMapping: 1,
    imageInputShapeMapping: 1
  }
}
